{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the App using LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangServe** is a component of the LangChain framework designed to help developers deploy language model applications as web services easily and efficiently. In simpler terms, it turns your language-based applications (like chatbots, translators, or any other app using language models) into web services that can be accessed over the internet through URLs.\n",
    "\n",
    "Here’s a step-by-step explanation on how to deploy a LangChain app using LangServe:\n",
    "\n",
    "1. **Develop Your App**: First, you create your language model application using LangChain. This involves setting up the language model, defining how it should process inputs and outputs, and possibly integrating with other systems or APIs.\n",
    "\n",
    "2. **Prepare for Deployment**:\n",
    "   - Ensure that your app is well-tested locally and behaves as expected.\n",
    "   - Configure any necessary environment variables, such as API keys or service endpoints, which the app will need to function properly once deployed.\n",
    "\n",
    "3. **Set Up LangServe**:\n",
    "   - LangServe acts as a server environment for your LangChain app. You'll need to configure LangServe settings specific to your application, such as the port number on which the server should run and any specific route paths (URLs) that should be handled by your app.\n",
    "\n",
    "4. **Deploy the Application**:\n",
    "   - Using tools and commands provided by LangChain, you can start the LangServe server with your application loaded on it. This makes your app available on a specified port of your server machine.\n",
    "   - If you need your service to be accessible publicly over the internet (not just locally), you might need to deploy it on a cloud platform or a dedicated server with a public IP address.\n",
    "\n",
    "5. **Access the App via URLs**:\n",
    "   - Once deployed, your application can be accessed through web URLs. This could be for direct user interaction (like a web-based chat interface) or for integration with other systems (like APIs that other software can call to use your app).\n",
    "\n",
    "LangServe effectively simplifies the process of making your LangChain apps accessible as standard web services, which can be a powerful way to deploy AI-driven language applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will need to install langserve from terminal**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See the file 001-simpleTranslator.py in your code editor.\n",
    "* Remember to create .gitignore file and include .env there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI\n",
    "# from langserve import add_routes\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# import os\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     ('system', system_template),\n",
    "#     ('user', '{text}')\n",
    "# ])\n",
    "\n",
    "# chain = prompt_template | llm | parser\n",
    "\n",
    "# app = FastAPI(\n",
    "#   title=\"simpleTranslator\",\n",
    "#   version=\"1.0\",\n",
    "#   description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    "# )\n",
    "\n",
    "# add_routes(\n",
    "#     app,\n",
    "#     chain,\n",
    "#     path=\"/chain\",\n",
    "# )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "\n",
    "#     uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sse_starlette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from threading import Thread\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    ChatOpenAI(),\n",
    "    path=\"/openai\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def read_root():\n",
    "        return {\"Hello\": \"World\"}\n",
    "\n",
    "    # Function to run the Uvicorn server in a thread\n",
    "    def run_server():\n",
    "        uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "\n",
    "    # Start the server in a separate thread\n",
    "    thread = Thread(target=run_server)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Playground to check the app: [http://127.0.0.1:8000/](http://127.0.0.1:8000/)\n",
    "* API routes:\n",
    "    * [http://127.0.0.1:8000/openai](http://127.0.0.1:8000/openai)\n",
    "    * [http://127.0.0.1:8000/chain](http://127.0.0.1:8000/chain)\n",
    "* FastAPI API documentation: [http://127.0.0.1:8000/docs#/](http://127.0.0.1:8000/docs#/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And finally, let's see how we can use the LangServe Playground from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"Spanish\", \"text\": \"Generative AI is a bigger opportunity than Internet\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangServe vs. FastAPI\n",
    "Deploying a LangChain app with LangServe or FastAPI involves similar basic principles—both methods aim to serve your application over the web—but they differ in their level of specialization and the features they offer. Here's a simple explanation of the main differences between these two deployment options:\n",
    "\n",
    "#### FastAPI\n",
    "\n",
    "1. **General-Purpose Framework**: FastAPI is a modern, fast (high-performance) web framework for building APIs with Python. It's designed to be simple to use but powerful in capabilities, supporting the development of robust APIs and web applications.\n",
    "\n",
    "2. **Flexibility**: FastAPI provides extensive flexibility in how you structure your application. It allows for detailed customization of request handling, response formatting, and middleware integration, making it suitable for a wide variety of web services beyond just language applications.\n",
    "\n",
    "3. **Manual Setup**: When deploying a LangChain app with FastAPI, you need to manually set up the routing, request handling, and integration with LangChain. This involves writing more boilerplate code and handling more configuration details.\n",
    "\n",
    "4. **Community and Ecosystem**: FastAPI has a large developer community and a rich ecosystem of plugins and tools, which can be advantageous for solving common web development problems and integrating with other technologies.\n",
    "\n",
    "#### LangServe\n",
    "\n",
    "1. **Specialized for LangChain**: LangServe is tailored specifically for deploying LangChain applications. This specialization means it comes with built-in configurations and setups optimized for language model applications, reducing the need to manually configure many aspects of deployment.\n",
    "\n",
    "2. **Simplicity and Convenience**: LangServe aims to simplify the process of turning your LangChain model into a web service. It abstracts away many of the lower-level details of web service configuration, allowing you to focus more on developing the language model itself.\n",
    "\n",
    "3. **Integrated Tools**: Since LangServe is designed to work seamlessly with LangChain, it often includes tools and features that specifically support language model operations, such as handling different types of language inputs and outputs more effectively.\n",
    "\n",
    "4. **Limited Flexibility**: While offering simplicity, LangServe may not provide as much flexibility as FastAPI in terms of general web development capabilities. It's optimized for a specific type of application, which might limit its utility outside of deploying language models.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Choose FastAPI** if you need a more flexible, general-purpose approach that can handle a wide variety of web services, require detailed customization, or want to integrate closely with other web technologies.\n",
    "- **Choose LangServe** if you are focused on deploying LangChain applications quickly and efficiently, prefer simplicity over flexibility, and do not require the extensive capabilities of a full-fledged web framework.\n",
    "\n",
    "Each option has its strengths and is best suited to different types of projects and developer preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wanted to build this application in a foreign language, meaning we communicate with the LLM model in a foreign language instead of in English?\n",
    "* See this in 002-simpleTranslatorBuiltInSpanish.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the LangChain app so that it translates from Spanish to English and communicates with the language model (LLM) in Spanish, you'll need to make several key changes to the prompt template.\n",
    "\n",
    "In this adjusted version:\n",
    "- The `system_template` is set to ask for translations into English but is phrased in Spanish to match the language model's settings.\n",
    "- The FastAPI application details have been updated to reflect the purpose of translating from Spanish to English.\n",
    "\n",
    "This will set up an API that receives text in Spanish, asks the model to translate it into English, and then serves the English translation via an endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-demos-9seDwfcA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
