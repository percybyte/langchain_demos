{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Principales Runnables en LCEL\n",
    "\n",
    "Entro los principales Runnables que nos otorga LangChain, tenemos:\n",
    "1. RunnablePassthrough: No altera el dato de entrada, útil para enviar datos sin procesamiento.\n",
    "2. RunnableLambda: Ejecuta una función personalizada, ideal para lógica específica.\n",
    "3. RunnableParallel: Ejecuta múltiples Runnables en paralelo, aumentando eficiencia.\n",
    "4. RunnableBranch: Selecciona Runnables según condiciones, útil para bifurcaciones en el flujo de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "model_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "* No hace nada con los datos de entrada.\n",
    "* Veámoslo con un ejemplo muy simple: una cadena con solo 'RunnablePassthrough()' generará la entrada original sin ninguna modificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abram'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough()\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "* Para utilizar una función personalizada dentro de una cadena LCEL, debemos encapsularla con RunnableLambda.\n",
    "* Definamos una función muy simple para crear apellidos rusos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abramovich'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\"\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "* Usaremos RunnableParallel() para ejecutar tareas en paralelo.\n",
    "* Este es probablemente el Runnable más importante y útil de LangChain.\n",
    "* En la siguiente cadena, RunnableParallel ejecutará estas dos tareas en paralelo:\n",
    "* operación_a usará RunnablePassthrough.\n",
    "* operación_b usará RunnableLambda con la función russian_lastname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Abram', 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En lugar de utilizar RunnableLambda, ahora vamos a utilizar una función lambda e invocaremos la cadena con dos entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordam', 'name': 'Abram'},\n",
       " 'soccer_player': 'Abramovich'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": lambda x: x[\"name\"]+\"ovich\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos agregar más Runnables a la cadena\n",
    "* En el siguiente ejemplo, el indicador Runnable tomará la salida de RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roman Abramovich, the Russian billionaire and former owner of Chelsea Football Club, is known not just for his wealth and business ventures but also for his unique collection of art. One particularly curious fact about him is that he owns one of the largest private collections of contemporary art in the world, including works by renowned artists such as Francis Bacon, Lucian Freud, and Damien Hirst. His passion for art is reflected in his investments and his involvement in various cultural initiatives, showcasing a side of him that goes beyond his business acumen and sports interests.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | model_llm | output_parser\n",
    "\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como viste, el indicador Runnable tomó \"Abramovich\", la salida de RunnableParallel, como valor para la variable \"soccer_player\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veamos un uso más avanzado de RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario que instalemos la siguiente librería:\n",
    "```\n",
    "poetry add faiss-cpu\n",
    "```\n",
    "¿Qué hace esta librería?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una base de datos vectorial usando el motor de búsqueda FAISS\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera ha formado a más de 10.000 antiguos alumnos de todos los continentes y de las mejores empresas\"], embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los alumnos de AI Accelera son más de 10.000 antiguos alumnos que provienen de todos los continentes y de las mejores empresas.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Responde a la pregunta basada en el siguiente contexto: {context}\n",
    "Pregunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Armamos el prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model_llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"¿Quienes son los alumnos de AI Accelera?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante: la sintaxis de RunnableParallel puede tener varias variaciones.\n",
    "Al componer un RunnableParallel con otro Runnable, no es necesario encapsularlo en la clase RunnableParallel. Dentro de una cadena, las siguientes tres sintaxis son equivalentes:\n",
    "* `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "* `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "* `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de itemgetter con RunnableParallel\n",
    "* Cuando se llama al LLM con varias variables de entrada diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Argh! AI Accelera ha entrenado a más de 5.000 viejos marineros, ¡eso es un montón de almas!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera ha capacitado a más de 5.000 antiguos alumnos de Enterprise.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Responde la pregunta basada solo en el siguiente contexto: {context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Responde en el siguiente lenguaje: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model_llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"Cuantas personlas han sido entrenadas por AI Accelera?\", \"language\": \"Español Pirata\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch: Router Chain\n",
    "* Una RunnableBranch es un tipo especial de ejecutable que le permite definir un conjunto de condiciones y ejecutables para ejecutar en función de la entrada.\n",
    "* **Una RunnableBranch se inicializa con una lista de pares (condición, ejecutable) y un ejecutable predeterminado**. Selecciona qué rama pasa a cada condición la entrada con la que se invoca. Selecciona la primera condición que se evalúa como Verdadera y ejecuta el ejecutable correspondiente a esa condición con la entrada.\n",
    "* Para usos avanzados, una [función personalizada](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/) puede ser una mejor alternativa que RunnableBranch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejemplo avanzado puede clasificar y responder a las preguntas de los usuarios en función de temas específicos, como rock, política, historia, deportes o consultas generales. **Utiliza algunos temas nuevos que explicaremos en la siguiente lección**. A continuación, se incluye una explicación simplificada de cada parte:\n",
    "\n",
    "1. **Plantillas de indicaciones**: cada plantilla está diseñada para un tema específico:\n",
    "- **rock_template**: configurada para preguntas relacionadas con el rock and roll.\n",
    "- **politics_template**: diseñada para responder preguntas sobre política.\n",
    "- **history_template**: diseñada para consultas relacionadas con la historia.\n",
    "- **sports_template**: configurada para responder preguntas relacionadas con los deportes.\n",
    "- **general_prompt**: una plantilla general para consultas que no se ajustan a las categorías específicas.\n",
    "\n",
    "Cada plantilla incluye un marcador de posición `{input}` donde se insertará la pregunta real del usuario.\n",
    "\n",
    "2. **RunnableBranch**: este es un mecanismo de ramificación que selecciona qué plantilla usar en función del tema de la pregunta. Evalúa condiciones (como `x[\"topic\"] == \"rock\"`) para determinar el tema y utiliza la plantilla de solicitud adecuada.\n",
    "\n",
    "3. **Clasificador de temas**: una clase de Pydantic que clasifica el tema de la pregunta de un usuario en una de las categorías predefinidas (rock, política, historia, deportes o general).\n",
    "\n",
    "4. **Cadena de clasificadores**:\n",
    "- **Cadena**: procesa la entrada del usuario para predecir el tema.\n",
    "- **Analizador**: extrae el tema predicho de la salida del clasificador.\n",
    "\n",
    "5. **RunnablePassthrough**: este componente introduce la entrada del usuario y el tema clasificado en RunnableBranch.\n",
    "\n",
    "6. **Cadena final**:\n",
    "- La entrada del usuario se procesa primero para clasificar su tema.\n",
    "- Luego, se selecciona la solicitud adecuada en función del tema clasificado.\n",
    "- La solicitud seleccionada se utiliza para formular una pregunta que luego se envía a un modelo (como ChatOpenAI).\n",
    "- La respuesta del modelo se analiza como una cadena y se devuelve.\n",
    "\n",
    "7. **Ejecución**:\n",
    "- Se invoca la cadena con una pregunta de muestra, \"¿Quién fue Napoleón Bonaparte?\".\n",
    "- En función de la clasificación, selecciona la plantilla adecuada, genera una consulta al modelo de chat y procesa la respuesta.\n",
    "\n",
    "El sistema crea efectivamente un generador de respuestas dinámicas que ajusta la forma en que responde en función del tema de la consulta, haciendo uso de conocimientos especializados para diferentes temas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_template = \"\"\"Eres un profesor de rock and roll muy inteligente. \\\n",
    "Eres muy bueno respondiendo preguntas sobre rock and roll de una manera concisa\\\n",
    "y fácil de entender.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"Eres un profesor de política muy bueno. \\\n",
    "Eres muy bueno respondiendo preguntas sobre política..\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"Eres un profesor de historia muy bueno. \\\n",
    "Tienes un excelente conocimiento y comprensión de personas,\\\n",
    "eventos y contextos de una variedad de períodos históricos.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" Eres profesor de deportes.\\\n",
    "Eres muy bueno respondiendo preguntas sobre deportes.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\n",
    "\"Eres un asistente útil. Responde la pregunta con la mayor precisión posible.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Clasificar el tema de la pregunta del usuario\"\n",
    "\n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "    \"El tema de la pregunta del usuario. Uno de 'rock', 'politics', 'history', 'sports' o 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"})\n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `classifier_function` clasifica o categoriza el tema de la pregunta de un usuario en categorías específicas como \"rock\", \"política\", \"historia\" o \"deportes\". Así es como funciona en términos simples:\n",
    "\n",
    "1. **Conversión a función**: Convierte la clase Pydantic `TopicClassifier`, que es un sistema de clasificación predefinido, en una función que se puede usar fácilmente con LangChain. Este proceso de conversión implica encapsular la clase para que pueda integrarse y ejecutarse dentro de un modelo OpenAI.\n",
    "\n",
    "2. **Detección de tema**: Cuando ingresa una pregunta, esta función analiza el contenido de la pregunta para determinar a qué categoría o tema pertenece. Busca palabras clave o patrones que coincidan con temas específicos. Por ejemplo, si la pregunta es sobre una banda de rock, el clasificador identificaría el tema como \"rock\".\n",
    "\n",
    "3. **Salida**: La función genera el tema identificado como una etiqueta simple, como \"rock\" o \"historia\". Esta etiqueta es utilizada por otras partes de LangChain para decidir cómo manejar la pregunta, como elegir la plantilla correcta para formular una respuesta.\n",
    "\n",
    "En esencia, la `classifier_function` actúa como un filtro inteligente que ayuda al sistema a entender qué tipo de pregunta se está haciendo para que pueda responder de manera más precisa y relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain)\n",
    "    | prompt_branch\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Napoleón Bonaparte fue un líder militar y político francés que se convirtió en emperador de Francia en el siglo XIX. Nacido en Córcega en 1769, Napoleón ascendió rápidamente en las filas del ejército durante la Revolución Francesa y se convirtió en líder militar durante las Guerras Napoleónicas. Conocido por su genio militar y su ambición política, Napoleón conquistó gran parte de Europa occidental y central, estableciendo un vasto imperio francés. Sin embargo, su imperio eventualmente colapsó y fue derrotado en la Batalla de Waterloo en 1815. A pesar de su caída, Napoleón sigue siendo una figura influyente en la historia europea y mundial.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\"input\": \"¿Quién es Napoleón Bonaparte?\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-demos-9seDwfcA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
