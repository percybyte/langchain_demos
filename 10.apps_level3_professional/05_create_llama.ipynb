{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9c4006-062e-41c8-9516-148232299b09",
   "metadata": {},
   "source": [
    "## Create-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98879c-4281-4f10-bf0f-9679975b7075",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Command-line tool to generate a full-stack LlamaIndex application for RAG.\n",
    "    * Chatbot with private documents.\n",
    "    * But the private documents are loaded manually in the folder /data. So it does not have a database nor CRUD functionality. \n",
    "* Currently supporting three backend options:\n",
    "    * Next.js serverless\n",
    "    * Express\n",
    "    * Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63097484-9046-4be0-be77-58254d57ddd3",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* [From LlamaIndex Blog](https://blog.llamaindex.ai/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0f0ee-ac28-4a2f-b35e-14c2444c92a5",
   "metadata": {},
   "source": [
    "## Option 1: Next.js serverless\n",
    "* Simplest version to deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125ca23-f660-46fb-9030-8c58f5d2b146",
   "metadata": {},
   "source": [
    "#### Local installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00069c0-d96c-474f-b4c5-873c87efb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npx create-llama@latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bd51e2-b091-44cd-ae29-ed897e2af88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload your private document in /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62beb03-287e-4869-8a4c-496645f0b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npm run generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1815ef9-1a02-497a-b9d8-ddfe10d0df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npm run dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3788b-b623-4942-ba9a-a7c8010b8cb0",
   "metadata": {},
   "source": [
    "Test the app locally in localhost:3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629f558-c0f8-4fc8-8802-1cdd11802535",
   "metadata": {},
   "source": [
    "#### Folder structure\n",
    "* .env with your OpenAI API Key\n",
    "* add .env in the .gitignore file\n",
    "* main logic in /app\n",
    "    * page.tsx: home page with 2 components (Header and ChatSection)\n",
    "    * main llamaindex logic in app/api/engine/index.ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f836a71-3b12-47b2-a2a7-f5bd1ccc906b",
   "metadata": {},
   "source": [
    "#### Deployment to Vercel\n",
    "* Create new github repo\n",
    "* Push to github\n",
    "* Import from Vercel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf058a63-010e-4235-b434-31569f1b1e5b",
   "metadata": {},
   "source": [
    "#### Got a deployment error in Vercel\n",
    "* Just removed the multimodal line that was causing the issue.\n",
    "* Commit changes to github and deploy again in Vercel.\n",
    "* All good now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287c03c-7615-4558-83cd-ee9930bad9f6",
   "metadata": {},
   "source": [
    "./app/components/chat-section.tsx:31:21\n",
    "* Type error: This comparison appears to be unintentional because the types '\"gpt-3.5-turbo\"' and '\"gpt-4-vision-preview\"' have no overlap.\n",
    "*  29 |         handleInputChange={handleInputChange}\n",
    "* 30 |         isLoading={isLoading}\n",
    "* > 31 |         multiModal={MODEL === \"gpt-4-vision-preview\"}\n",
    "* Just delete line 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfe2f9-d832-4c38-b90e-d5e092860739",
   "metadata": {},
   "source": [
    "## Option 2: frontend with Nextjs, backend with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f6ed9-573f-4ce3-8a28-5155718b22c4",
   "metadata": {},
   "source": [
    "#### Local installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c501b5be-4e3b-4a2d-ad0d-d478bdf1a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npx create-llama@latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26390105-7bd6-4c43-9b32-9d3595194332",
   "metadata": {},
   "source": [
    "Select FastAPI Option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23330bf6-77c1-476d-8aab-f1480f48eec5",
   "metadata": {},
   "source": [
    "#### Folder structure\n",
    "* backend and frontend separated in two main folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae82f40-c6d0-4c99-a710-acb0acd98a93",
   "metadata": {},
   "source": [
    "#### Deployment of both frontend and backend to Render.com\n",
    "* Load full-stack app to new github repo\n",
    "* Render.com > New project > Static site\n",
    "    * select github repo\n",
    "    * root directory: frontend\n",
    "    * build command: frontend/npm install; npm build run\n",
    "    * publish directory: frontend/out\n",
    "    * environment variables: NODE_VERSION=20\n",
    "    * deploy the static frontend\n",
    "    * got the same error than in Vercel deployment: solved the same way\n",
    "\n",
    "* Render.com > New project > New web service\n",
    "    *  select the same github repo than in the previous step\n",
    "    *  root directory: backend\n",
    "    *  build command: backend/poetry install\n",
    "    *  start command: backend/uvicorn main:app --host 0.0.0.0 --port 10000\n",
    "    *  environment variables:\n",
    "        *  OPENAI_API_KEY=...\n",
    "        *  PYTHON_VERSION=3.11.6\n",
    "    * copy the URL of the backend\n",
    "\n",
    "* Go to frontend project and enter the URL of the backend as environment variable there:\n",
    "    * NEXT_PUBLIC_CHAT_API=yourBackendURL/api/chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8605da-7429-493a-a8fd-65364458254b",
   "metadata": {},
   "source": [
    "#### Click on the URL of the frontend project to check the full stack app.\n",
    "* In case it does not work, try this: click on Clear Build Cache & Deploy (this option is in the drop-down of the Manual Deploy blue button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010037f-f5e8-495c-b482-4a69a035fff8",
   "metadata": {},
   "source": [
    "[Live app](https://starter-nextjs-fastapi.onrender.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f2ae2-1d33-4152-a13b-6b3a2a582702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
