{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como construir un chatbot simple con memoria almacenada usando LangChain\n",
    "* El chatbot podrá mantener una conversación\n",
    "* Recordará interacciones anteriores\n",
    "* Podrá almacenar memoria en una archivo JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truco para evitar las desagradables advertencias de desuso de LangChain\n",
    "\n",
    "En este ejercicio, vamos a usar la cadena heredada de LangChain (LLMChain). Aunque funciona bien, se nos va a estar mostrando una advertencia de desuso. Para poder evitarlo, vamos a ingresar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementación inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Creamos el modelo a utilizar\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Creamos nuestro formateador de salida\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messagesToTheChatbot = [\n",
    "    HumanMessage(content=\"Mi color favorito es el azul.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llamamos al modelo de Chat (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡El azul es un color hermoso! A menudo se asocia con la calma, la tranquilidad y la confianza. ¿Tienes algún tono de azul favorito o alguna razón especial por la que te gusta?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 14, 'total_tokens': 55, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-56a298df-8e23-43f6-8cf1-b84b66e64217-0', usage_metadata={'input_tokens': 14, 'output_tokens': 41, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatModel.invoke(messagesToTheChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vamos a revisar si el Chatbot recuerda nuestro color favorito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Lo siento, pero no tengo la capacidad de recordar información personal de conversaciones anteriores. Si me dices cuál es tu color favorito, estaré encantado de recordarlo durante esta conversación.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 17, 'total_tokens': 53, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-9b249f54-5cf0-4235-a46e-975993343973-0', usage_metadata={'input_tokens': 17, 'output_tokens': 36, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatModel.invoke([\n",
    "    HumanMessage(content=\"¿Recuerdas cual es mi color favorito?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como podemos ver el chatbot, no tiene la capacidad de recordar nuestro color favorito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregando memoria a nuestro ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Ahora los paquetes para manejar la memoria\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear la memoria del chatbot\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=FileChatMessageHistory('messages.json'), # Guardamos en un archivo json la memoria\n",
    "    memory_key=\"messages\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro Prompt Template\n",
    "#TODO: Validemos la implementación de ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['content','messages'],\n",
    "    messages=[\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos nuestra cadena\n",
    "chain = LLMChain(\n",
    "    llm=chatModel,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Hola!!',\n",
       " 'messages': [],\n",
       " 'text': '¡Hola! ¿Cómo puedo ayudarte hoy?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciamos a interactuar con el chatbot\n",
    "chain.invoke(\"Hola!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Mi nombre es Percy Tejada',\n",
       " 'messages': [HumanMessage(content='Hola!!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='¡Hola! ¿Cómo puedo ayudarte hoy?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': '¡Encantado de conocerte, Percy! ¿En qué puedo ayudarte hoy?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Mi nombre es Percy Tejada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '¿Cual es mi nombre?',\n",
       " 'messages': [HumanMessage(content='Hola!!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='¡Hola! ¿Cómo puedo ayudarte hoy?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Mi nombre es Percy Tejada', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='¡Encantado de conocerte, Percy! ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Tu nombre es Percy Tejada. ¿Hay algo más con lo que te pueda ayudar?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"¿Cual es mi nombre?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El historial de la conversación se estará guardando en el archivo `messages.json`.\n",
    "* Este es solo un ejemplo. En una aplicación real probablemente no guardemos la memoria en un archivo json.\n",
    "* Recuerda que la ventana de contexto es limitada y esto afectará el costo de uso de la API ChatGPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-demos-9seDwfcA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
