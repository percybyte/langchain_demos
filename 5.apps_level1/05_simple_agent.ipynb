{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo crear un Agente LLM sencillo con LangGraph\n",
    "* Agente LLM con memoria que utiliza herramientas muy básicas.\n",
    "\n",
    "Los modelos de lenguaje no pueden hacer nada por sí solos; solo pueden crear texto en función de lo que se les pida. Sin embargo, LangChain permite a las personas crear agentes (piense en ellos como sistemas inteligentes o ayudantes) que utilizan modelos de lenguaje para pensar y decidir qué hacer a continuación.\n",
    "\n",
    "Así es como funciona:\n",
    "1. **Utilice el modelo de lenguaje como cerebro**: el agente utiliza el modelo de lenguaje para determinar qué acciones debe realizar, en función de la información que tiene o de lo que se le pida que haga.\n",
    "2. **Actuación**: después de tomar una decisión, el agente procede a realizar esas acciones.\n",
    "3. **Aprendizaje y ajuste**: una vez realizadas las acciones, los resultados se pueden devolver al modelo de lenguaje. Esto ayuda al modelo a verificar si todo está completo o si necesita hacer algo más.\n",
    "\n",
    "Básicamente, LangChain ayuda a convertir un modelo de lenguaje de una herramienta para escribir y responder a un sistema que puede actuar y reaccionar, casi como un cerebro robótico muy simple.\n",
    "\n",
    "Aquí crearemos un agente que pueda interactuar con un motor de búsqueda. Podrás hacerle preguntas a este agente, observarlo mientras llama a la herramienta de búsqueda y tener conversaciones con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Creamos el modelo a utilizar\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Creamos nuestro formateador de salida\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes\n",
    "* Los agentes utilizan los LLM como motores de razonamiento para determinar qué acciones tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente que utiliza herramientas\n",
    "* Para este agente básico, utilizaremos solo una herramienta. En los próximos proyectos avanzados, aprenderá a utilizar agentes con varias herramientas.\n",
    "* **Nuestra herramienta de elección será Tavily**, un motor de búsqueda.\n",
    "#### Clave API de Tavily\n",
    "* Deberá agregar su clave API de Tavily en el archivo .env. Obtenga una clave registrándose en su [sitio web](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si está utilizando el shell de poetry precargado, no necesita instalar el siguiente paquete porque ya está precargado para usted, sino deberá instalarlo con:\n",
    "\n",
    "`#pip install langchain-community`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search.invoke(\"¿Quién fue la estrella de la Eurocopa en 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Para permitir que este modelo realice llamadas a herramientas, usamos .bind_tools** para brindarle al modelo de lenguaje conocimiento de estas herramientas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = chatModel.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear el agente\n",
    "* Usaremos LangGraph para construir el agente.\n",
    "* **Tenga en cuenta que a continuación pasamos el modelo de chat original, no el llm_with_tools que creamos más tarde**. Esto se debe a que create_tool_calling_executor llamará a .bind_tools por nosotros en forma interna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(chatModel, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar el agente\n",
    "* Primero, intentémoslo con .invoke():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"¿Dónde se jugó la Euro 2024?\")]})\n",
    "\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ahora probémoslo con .stream():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"When and where will it be the 2024 Eurocup final match?\")]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar memoria\n",
    "* Agregar memoria en LangGraph es muy similar a lo que hicimos con LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creemos nuestro nuevo agente con memoria y establezcamos un thread_id para que el agente pueda crear una memoria para cada sesión como lo hicimos con nuestra aplicación RAG conversacional anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(chatModel, tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probemos ahora este nuevo agente con .stream():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"¿Quién ganó la Euro 2024?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cambiemos el thread_id y veamos qué sucede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"002\"}}\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"¿Sobre que jugador estamos hablando?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como puedes observar al cambiar el threat_id iniciamos una memoria de conversación diferente, por lo que nuestra app no ​​recuerda la interacción anterior y en lugar de darnos la respuesta correcta decidió alucinar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-demos-9seDwfcA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
